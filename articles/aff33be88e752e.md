---
title: "OpenCALM-7Bã‚’M2 Macã§å‹•ã‹ã™"
emoji: "ğŸ’¬"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: [python, AI, LLM]
published: true
---


# ã¯ã˜ã‚ã«
ã“ã®è¨˜äº‹ã§ã¯ã€CyberAgentç¤¾ãŒå…¬é–‹ã—ãŸLLMã§ã‚ã‚‹OpenCALM-7Bã®ãƒ¢ãƒ‡ãƒ«ã‚’M2 Macã®ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œã—ãŸTipsã‚’è¨˜è¼‰ã—ã¾ã™ã€‚

# OpenCALMã¨ã¯

OpenCALMã¯CyberAgentç¤¾ãŒé–‹ç™ºã—ãŸãƒ‡ã‚³ãƒ¼ãƒ€å‹ã®äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
å•†ç”¨åˆ©ç”¨å¯èƒ½ãªãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¨ãªã£ã¦ãŠã‚Šã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚

https://www.cyberagent.co.jp/news/detail/id=28817

OpenCALM-7Bãƒ¢ãƒ‡ãƒ«ã¯HuggingFaceã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

https://huggingface.co/cyberagent/open-calm-7b

# PCã‚¹ãƒšãƒƒã‚¯
å®Ÿè¡Œã—ãŸPCã®ã‚¹ãƒšãƒƒã‚¯ã¯ä»¥ä¸‹ã®ã¨ãŠã‚Šã§ã™ã€‚

```
ãƒ¡ãƒ¢ãƒªï¼š64GB
ãƒãƒƒãƒ—ï¼šApple M2 Max
OSï¼šVentura 13.4.1 
```


# å®Ÿè¡Œæ‰‹é †

ãƒ­ãƒ¼ã‚«ãƒ«ã«`.ipynb`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸã€‚ï¼ˆä»®æƒ³ç’°å¢ƒã¯`Python 3.9.6`ã§ã™ï¼‰

## ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

ä»¥ä¸‹ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚

```python
!pip install transformers accelerate
```

## ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

æ¬¡ã«ã€ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
è‡ªèº«ã®ç’°å¢ƒã ã¨ç´„55åˆ†ç¨‹åº¦ã‹ã‹ã‚Šã¾ã—ãŸã€‚

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "cyberagent/open-calm-7b", 
    device_map="auto", 
    torch_dtype=torch.float16
)
tokenizer = AutoTokenizer.from_pretrained("cyberagent/open-calm-7b")

```

ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ¢ãƒ‡ãƒ«ã¯`~/.cache/huggingface/hub/models--cyberagent--open-calm-7b`é…ä¸‹ã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚
æ¬¡å›ä»¥é™ã¯ã“ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚

## ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æº–å‚™ã¨æ¨è«–ã®å®Ÿè¡Œ

```python

prompt = "Q:æ—¥æœ¬ã®é¦–éƒ½ã¯ã©ã“ã§ã™ã‹ï¼Ÿ A.ã€Œ"

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
with torch.no_grad():
    tokens = model.generate(
        **inputs,
        max_new_tokens=64,
        do_sample=True,
        temperature=0.1,
        pad_token_id=tokenizer.pad_token_id,
    )
    
output = tokenizer.decode(tokens[0], skip_special_tokens=True)
print(output)

```

å®Ÿè¡Œå¾Œã€ä»¥ä¸‹ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚

```bash
RuntimeError: MPS does not support cumsum op with int64 input.
```

ä»¥ä¸‹ã‚³ãƒ¡ãƒ³ãƒˆã‚’å‚è€ƒã«pytorchã‚’nightlyãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ã™ã‚‹ã¨ä¸Šè¨˜ã‚¨ãƒ©ãƒ¼ã¯è§£æ¶ˆã—ã¾ã—ãŸã€‚
https://github.com/pytorch/pytorch/issues/96610#issuecomment-1593230620

```
!pip install --upgrade --no-deps --force-reinstall --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu
```

ã‚¨ãƒ©ãƒ¼è§£æ¶ˆå¾Œã®å®Ÿè¡Œçµæœã€‚è³ªå•ã¨ç­”ãˆãŒç¹°ã‚Šè¿”ã—å‡ºåŠ›ã•ã‚Œã‚‹ã€‚
```
Q:æ—¥æœ¬ã®é¦–éƒ½ã¯ã©ã“ã§ã™ã‹ï¼Ÿ A.ã€Œæ±äº¬éƒ½ã€ã§ã™ã€‚Q:æ—¥æœ¬ã®é¦–éƒ½ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚A:æ±äº¬éƒ½ã§ã™ã€‚....
```

# å‚è€ƒ

https://note.com/npaka/n/n2185b422a2f2